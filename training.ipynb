{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"466f031195b6443681a58da7702013f8","deepnote_cell_type":"text-cell-h1"},"source":"# Homework 2","block_group":"2068b2549ceb43059eacefc5c97724fd"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207034945,"execution_millis":1735,"deepnote_to_be_reexecuted":false,"cell_id":"06eee04a48e1419bb2aa9d28b51c2ad6","deepnote_cell_type":"code"},"source":"import tensorflow as tf","block_group":"06eee04a48e1419bb2aa9d28b51c2ad6","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-02 14:50:35.045290: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-01-02 14:50:35.047102: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-02 14:50:35.086538: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-02 14:50:35.087250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-01-02 14:50:35.858539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"893a1ae2c1f24d5bb809c8dde51223c0","deepnote_cell_type":"text-cell-h2"},"source":"## Hyperparameters","block_group":"966267d0be574088b0288672ce90987c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2d019292f8c248d9abb1f600e2fe06b8","deepnote_cell_type":"text-cell-p"},"source":"We included the computation of the MFCC coefficients as a crucial step in the pre-processing phase.","block_group":"6ffe1d64c4044d96955b4c4ed08334b8"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207358008,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"6f28fcc0372943bb950b7c366a939567","deepnote_cell_type":"code"},"source":"PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.05,\n    'frame_step_in_s': 0.028,\n    'num_mel_bins': 20,\n    'lower_frequency': 20,\n    'upper_frequency': 5000,\n}\n\nMFCC_ARGS = {\n    **PREPROCESSING_ARGS,\n    'num_coefficients': 10,\n}\n\nTRAINING_ARGS = {\n    'batch_size': 10,\n    'initial_learning_rate': 1.e-1,\n    'end_learning_rate': 1.e-4,\n    'epochs': 40\n}\n\n","block_group":"5dfec686556344318886c03491a661a6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2d5183373fdf478bb2992bb9a7e3bd55","deepnote_cell_type":"text-cell-h2"},"source":"## Create datasets","block_group":"3a3462be33d744b88086dc9f36c4e9b6"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207143047,"execution_millis":188106,"deepnote_to_be_reexecuted":false,"cell_id":"15e68c0e70be43eca949ecdbc04971dc","deepnote_cell_type":"code"},"source":"train_data = tf.data.Dataset.list_files('yn-train/*').shuffle(buffer_size=1600)\ntest_data = tf.data.Dataset.list_files('yn-test/*')\n\nprint(\"Train-set size: \", len(train_data))\nprint(\"Test-set size: \", len(test_data))\n","block_group":"d30bc34cd6ba4509a6951f3882b5ba74","execution_count":null,"outputs":[{"name":"stdout","text":"Train-set size:  1600\nTest-set size:  200\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"595adfaebbf34eb6b73b6f53921e870f","deepnote_cell_type":"text-cell-h2"},"source":"## Pre-processing","block_group":"1d31ec85eef94fc59e57e0fd1beb96b0"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"7b83aef7b06b44069c469a22e2348751","deepnote_cell_type":"text-cell-p"},"source":"Modify the file preprocessing.py in order to calculate the MFCCs","block_group":"063a027f69534602a96e50522f8375a2"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207360700,"execution_millis":347,"deepnote_to_be_reexecuted":false,"cell_id":"423386893072446d8e818a57ea4ce786","deepnote_cell_type":"code"},"source":"from preprocessing import LABELS\nfrom preprocessing import AudioReader\nfrom preprocessing import MelSpectrogram\nfrom preprocessing import MFCC\n\n\naudio_reader = AudioReader(tf.int16, 16000)\nmel_spec_processor = MelSpectrogram(**PREPROCESSING_ARGS)\nmfccs = MFCC(**MFCC_ARGS)\n\ndef prepare_for_training(feature, label):\n    feature = tf.expand_dims(feature, -1)\n    label_id = tf.argmax(label == LABELS)\n    return feature, label_id\n\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\n\ntrain_ds = (train_data\n            .map(audio_reader.get_audio_and_label)\n            .map(mfccs.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size)\n            .cache())\ntest_ds = (test_data\n            .map(audio_reader.get_audio_and_label)\n            .map(mfccs.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size))\n\n","block_group":"c49be218995b4ef48cf8ed0344cfc7c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207362964,"execution_millis":3031,"deepnote_to_be_reexecuted":false,"cell_id":"ecb38e570403456ab831f6e2bee9e8b4","deepnote_cell_type":"code"},"source":"for example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","block_group":"bf456e7318ca40069021e296ce1d33c4","execution_count":null,"outputs":[{"name":"stdout","text":"Batch Shape: (10, 34, 10, 1)\nData Shape: (34, 10, 1)\nLabels: tf.Tensor([1 1 1 1 1 0 0 0 1 0], shape=(10,), dtype=int64)\n2024-01-02 14:56:05.221388: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"74f405c086ad45de93eb4c5e4595badd","deepnote_cell_type":"text-cell-h2"},"source":"## Create model","block_group":"56d1db2a0c9d4ee187ab089e8045528d"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"099779c129714909a75c89c22ee14ada","deepnote_cell_type":"text-cell-p"},"source":"We used a basic sequential model with 2 Convolutional layers and 2 dropout layers to have a better generalization on new data.","block_group":"57d3427147464169bca0c2de1874fb19"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207377094,"execution_millis":59,"deepnote_to_be_reexecuted":false,"cell_id":"486a36dfd1174827a5c911816bb4a249","deepnote_cell_type":"code"},"source":"import os\n\nalpha = 0.25\n\n\nref_model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=[34, 10, 1]),\n    tf.keras.layers.Conv2D(filters=(128*alpha), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(filters=(128*alpha*0.8), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=2),\n    tf.keras.layers.Softmax()\n])\n\nref_model.build()","block_group":"e38aa66dc0644724bccd5b7323388d85","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"832f419ec2d44d5bb19ecc75c0fd8441","deepnote_cell_type":"code"},"source":"ref_model.summary()","block_group":"2b12b9fa97554b79873e069f797cb3de","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9c7c5e8d68f24176aa0c18f591fc642a","deepnote_cell_type":"text-cell-h2"},"source":"## Train","block_group":"0771718b6a074b079d5f2ab3474124fa"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704207381706,"execution_millis":484319,"deepnote_to_be_reexecuted":false,"cell_id":"b8c3841207ee4df5a1278fb1185f9228","deepnote_cell_type":"code"},"source":"from tensorflow.keras.callbacks import EarlyStopping\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\nloss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.sparse_categorical_accuracy]  \nref_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nearly_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n\nhistory = ref_model.fit(train_ds, epochs=epochs, callbacks=early_stopping, shuffle=True)\n","block_group":"3061827d1b33418fb0cac6e5ac6468d6","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/40\n160/160 [==============================] - 391s 2s/step - loss: 0.5192 - sparse_categorical_accuracy: 0.7638\nEpoch 2/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.3740 - sparse_categorical_accuracy: 0.8500\nEpoch 3/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.3276 - sparse_categorical_accuracy: 0.8763\nEpoch 4/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.2982 - sparse_categorical_accuracy: 0.8919\nEpoch 5/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.2468 - sparse_categorical_accuracy: 0.9125\nEpoch 6/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.2184 - sparse_categorical_accuracy: 0.9162\nEpoch 7/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.2186 - sparse_categorical_accuracy: 0.9150\nEpoch 8/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1812 - sparse_categorical_accuracy: 0.9356\nEpoch 9/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1618 - sparse_categorical_accuracy: 0.9388\nEpoch 10/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1586 - sparse_categorical_accuracy: 0.9431\nEpoch 11/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1328 - sparse_categorical_accuracy: 0.9500\nEpoch 12/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9469\nEpoch 13/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1248 - sparse_categorical_accuracy: 0.9569\nEpoch 14/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9538\nEpoch 15/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1041 - sparse_categorical_accuracy: 0.9625\nEpoch 16/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.1015 - sparse_categorical_accuracy: 0.9600\nEpoch 17/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0897 - sparse_categorical_accuracy: 0.9694\nEpoch 18/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0870 - sparse_categorical_accuracy: 0.9644\nEpoch 19/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0882 - sparse_categorical_accuracy: 0.9638\nEpoch 20/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0745 - sparse_categorical_accuracy: 0.9675\nEpoch 21/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0773 - sparse_categorical_accuracy: 0.9663\nEpoch 22/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9769\nEpoch 23/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9725\nEpoch 24/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9731\nEpoch 25/40\n160/160 [==============================] - 2s 10ms/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9719\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8d49216a28bc497abf23c240fe4b5707","deepnote_cell_type":"text-cell-h2"},"source":"## Evaluation on Test-set","block_group":"9861a9f11fa04347bdf162bb7a6fb53f"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208258258,"execution_millis":55250,"deepnote_to_be_reexecuted":false,"cell_id":"2ccb5839642d40d7aa8ff46f2841ebd2","deepnote_cell_type":"code"},"source":"test_loss, test_accuracy = ref_model.evaluate(test_ds)\n\ntraining_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","block_group":"4c062da93b2247bfa1a1d60945cf7118","execution_count":null,"outputs":[{"name":"stdout","text":"20/20 [==============================] - 55s 3s/step - loss: 0.0599 - sparse_categorical_accuracy: 0.9900\nTraining Loss: 0.0683\nTraining Accuracy: 97.19%\n\nTest Loss: 0.0599\nTest Accuracy: 99.00%\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a0a1dfdd75894429ba7484db380f3fce","deepnote_cell_type":"text-cell-h2"},"source":"## Save model","block_group":"1fb39b1c9c4c441a8170a2f3e73639be"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208180680,"execution_millis":4006,"deepnote_to_be_reexecuted":false,"cell_id":"7a080f5a9e3c4e44b5b085366c4d485c","deepnote_cell_type":"code"},"source":"import os\n\nsaved_model_dir = f'./saved_models/model4' \nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nref_model.save(saved_model_dir)\n","block_group":"2411b88d15c44598b3281c37b50979ba","execution_count":null,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./saved_models/cavalloPazzoRabiot/assets\nINFO:tensorflow:Assets written to: ./saved_models/cavalloPazzoRabiot/assets\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6405ac9ea1944f32a6cbc673b8d13493","deepnote_cell_type":"text-cell-h2"},"source":"## convert model in tflite and quantize it ","block_group":"8b9cca31436d4be19888f0bb44874367"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"3867804250e447bebeafb8166b8abba0","deepnote_cell_type":"text-cell-p"},"source":"We applied the default optimizations provided by tf.lite ","block_group":"ca8668be8dc44b908e7f19794b3a2726"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208189656,"execution_millis":2325,"deepnote_to_be_reexecuted":false,"cell_id":"bc05f78748c747ff9a273d142d8ae1cb","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow_model_optimization.quantization.keras import quantize_model\n\nconverter_opt = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/model4')\nconverter_opt.optimizations = [tf.lite.Optimize.DEFAULT ]\ntflite_model_opt = converter_opt.convert()\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\ntflite_model_name = os.path.join(tflite_models_dir, 'model4.tflite')\n\nwith open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model_opt)\n","block_group":"d03038f08d0248f2a2e4270dae94c5bf","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-02 15:09:51.057684: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n2024-01-02 15:09:51.057718: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n2024-01-02 15:09:51.240448: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/cavalloPazzoRabiot\n2024-01-02 15:09:51.404083: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n2024-01-02 15:09:51.404116: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./saved_models/cavalloPazzoRabiot\n2024-01-02 15:09:51.406280: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n2024-01-02 15:09:51.407026: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n2024-01-02 15:09:51.768389: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ./saved_models/cavalloPazzoRabiot\n2024-01-02 15:09:51.777344: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 536907 microseconds.\n2024-01-02 15:09:51.788227: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"89a6a41d61e145b68d1b8c19fb15f4ef","deepnote_cell_type":"text-cell-h2"},"source":"## Reference latency","block_group":"669a043b37a54b0ca2f59b6bf9500781"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"ce633a3a9cf648df8a591704c91bddb0","deepnote_cell_type":"text-cell-p"},"source":"Define ref_model and convert it in TFlite (if it does not exist)","block_group":"df919d9ca0e847bab1e12b810d2a3864"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208194087,"execution_millis":171,"deepnote_to_be_reexecuted":false,"cell_id":"d69718456caa4f949b3171153910591b","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport os\n\n\nREF_PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n}\n\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\ntflite_model_name = os.path.join(tflite_models_dir, 'ref_model.tflite')\n\nif not os.path.exists(tflite_model_name):\n    ref_model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[49, 40, 1]),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=2),\n        tf.keras.layers.Softmax()\n    ])\n\n    ref_model.build()\n\n    saved_model_dir = f'./saved_models/ref_model'\n    if not os.path.exists(saved_model_dir):\n        os.makedirs(saved_model_dir)\n    ref_model.save(saved_model_dir)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/ref_model')\n    tflite_model = converter.convert()\n\n    with open(tflite_model_name, 'wb') as fp:\n        fp.write(tflite_model)","block_group":"2623d2822b134098b5fcf0d954e9fb1b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fce97d03cfe148debb91d54ec15172e1","deepnote_cell_type":"text-cell-p"},"source":"Evaluate latency on ref_mode.tflite","block_group":"6e9fde5bc7934c7ba0662fa6125b4370"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208196201,"execution_millis":1580,"deepnote_to_be_reexecuted":false,"cell_id":"2f121641b40b4b92873f2a7eeeb6fa87","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom time import time\nfrom preprocessing import MelSpectrogram\n\nmel_spec_processor = MelSpectrogram(**REF_PREPROCESSING_ARGS)\ninterpreter = tf.lite.Interpreter(model_path='tflite_models/ref_model.tflite')\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\naudio = tf.random.normal((16000,))\n\nref_latencies = []\n\nfor i in range(100):\n    start_preprocess = time()\n\n    log_mel_spectrogram = mel_spec_processor.get_mel_spec(audio)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n    interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    ref_latencies.append(end_inference - start_preprocess)\n\nmedian_ref_latency = np.median(ref_latencies)","block_group":"fb43d03c102a4faf9d6ee5727569b958","execution_count":null,"outputs":[{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8d403c8294764effbfac0b07c9697ab2","deepnote_cell_type":"text-cell-p"},"source":"Evaluate latency of our optimized model ","block_group":"fb0d377eef674a8b89b02b2589b99738"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208202560,"execution_millis":1259,"deepnote_to_be_reexecuted":false,"cell_id":"7a80cfb036864b1bb298f1af8d41a711","deepnote_cell_type":"code"},"source":"import numpy as np\nfrom time import time\nfrom preprocessing import MFCC\n\nmfcc = MFCC(**MFCC_ARGS)\ninterpreter_opt = tf.lite.Interpreter(model_path='tflite_models/model4.tflite') # insert our model name \ninterpreter_opt.allocate_tensors()\n\ninput_details = interpreter_opt.get_input_details()\noutput_details = interpreter_opt.get_output_details()\n\naudio = tf.random.normal((16000,))\n\noptimized_latencies = []\n\nfor i in range(100):\n\n    start_preprocess = time()\n\n    mfccs = mfcc.get_mfccs(audio)\n    mfccs = tf.expand_dims(mfccs, 0)\n    mfccs = tf.expand_dims(mfccs, -1)\n    interpreter_opt.set_tensor(input_details[0]['index'], mfccs)\n    interpreter_opt.invoke()\n    output = interpreter_opt.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    optimized_latencies.append(end_inference - start_preprocess)\n\nmedian_opt_latency = np.median(optimized_latencies)\n","block_group":"d62bc715b1614a9ab326ccafcaef63cd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"6343ecfb711446469636d9c41f011ecc","deepnote_cell_type":"text-cell-p"},"source":"Compute total latency saving ","block_group":"0ff22018cc0546d895875587a048962d"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208206999,"execution_millis":32,"deepnote_to_be_reexecuted":false,"cell_id":"a6eddd2174ef4baca2a38815abf6d070","deepnote_cell_type":"code"},"source":"latency_saving = 100 * (median_ref_latency-median_opt_latency) / median_ref_latency\n\nprint(\"Total latency saving: \", latency_saving)","block_group":"c3c818bf700245059cc775e3a4a37c6b","execution_count":null,"outputs":[{"name":"stdout","text":"Total latency saving:  38.33637990784112\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"794a8fe6c3674811828330d857ed72c3","deepnote_cell_type":"text-cell-h2"},"source":"## Save compressed model ","block_group":"539a57223ce64c01a0595fca6adab839"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208228217,"execution_millis":488,"deepnote_to_be_reexecuted":false,"cell_id":"9d198e45c0464b348e0935efc10af9a1","deepnote_cell_type":"code"},"source":"import zipfile\nimport os\n\ntflite_models_dir = './tflite_models'\n\ntflite_model_name = os.path.join(tflite_models_dir, f'model4.tflite')\ntflite_model_name\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)\n\ntflite_size = os.path.getsize(tflite_model_name) / 1024.0\nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0    \n\nprint(\"Original size \", tflite_size)\nprint()\nprint(\"New size: \", zipped_size)","block_group":"00f4516ff5034da0aa6c5dd5ba3297bf","execution_count":null,"outputs":[{"name":"stdout","text":"Original size  11.515625\n\nNew size:  9.9169921875\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"2a74aab015934c428f4ea21daa922957","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"7c8e60ee83534b1ba429646a3a8865f8"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=42dec124-2522-4d70-b81d-1e692b6f25c0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"b4999c2eb57941029915877df1d818b2","deepnote_execution_queue":[]}}